{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367199ad-1d30-41ec-953e-1b35e945ae47",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-02-24T05:50:14.329311Z",
     "iopub.status.busy": "2024-02-24T05:50:14.328553Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 24 05:50:14 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   26C    P8    27W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\u001b[33mWARNING: Skipping cmake as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--2024-02-24 05:50:16--  https://github.com/Kitware/CMake/releases/download/v3.22.1/cmake-3.22.1-linux-x86_64.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/9799ed74-63be-49c4-bf59-1ffe76891137?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240224T055016Z&X-Amz-Expires=300&X-Amz-Signature=80c01e0e2eb1384989d4e558e9225f18ee29b2871ce401b419e203e1c033660d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=537699&response-content-disposition=attachment%3B%20filename%3Dcmake-3.22.1-linux-x86_64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-02-24 05:50:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/9799ed74-63be-49c4-bf59-1ffe76891137?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240224%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240224T055016Z&X-Amz-Expires=300&X-Amz-Signature=80c01e0e2eb1384989d4e558e9225f18ee29b2871ce401b419e203e1c033660d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=537699&response-content-disposition=attachment%3B%20filename%3Dcmake-3.22.1-linux-x86_64.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 45345904 (43M) [application/octet-stream]\n",
      "Saving to: ‘cmake-3.22.1-linux-x86_64.tar.gz’\n",
      "\n",
      "cmake-3.22.1-linux- 100%[===================>]  43.25M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-02-24 05:50:16 (295 MB/s) - ‘cmake-3.22.1-linux-x86_64.tar.gz’ saved [45345904/45345904]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#VG起動プログラム\n",
    "\n",
    "!nvidia-smi\n",
    "# need reinstall cmake for pyopenjtalk\n",
    "!pip uninstall -y cmake\n",
    "!wget https://github.com/Kitware/CMake/releases/download/v3.22.1/cmake-3.22.1-linux-x86_64.tar.gz\n",
    "!tar xf cmake-3.22.1-linux-x86_64.tar.gz\n",
    "!rm cmake-3.22.1-linux-x86_64.tar.gz\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += \":/content/cmake-3.22.1-linux-x86_64/bin\"\n",
    "\n",
    "!apt-get install -y espeak\n",
    "!pip install kaleido cohere openai tiktoken typing-extensions==4.5.0 fastapi python-multipart uvicorn tensorflow-probability==0.20.1 gradio==3.34.0\n",
    "\n",
    "!git clone https://github.com/jackal-mitani/voice-generator-webui.git\n",
    "%cd /notebooks/voice-generator-webui\n",
    "!chmod +x setup.sh\n",
    "!./setup.sh\n",
    "\n",
    "!pip install -U gradio\n",
    "!pip install httpx==0.24.1\n",
    "!pip install tensorboardX\n",
    "!pip install protobuf==3.20\n",
    "!pip install -U lxml\n",
    "!pip install -U langchain\n",
    "!pip install -U pydantic\n",
    "!pip install -U langchain-community\n",
    "!pip install SQLAlchemy==2.0.1\n",
    "!pip install faiss-gpu\n",
    "\n",
    "# Add MIT RVC model\n",
    "# ref| https://chihaya369.booth.pm/items/4701666\n",
    "!mkdir -p vc/models/AISO-HOWATTO\n",
    "!wget -P vc/models/AISO-HOWATTO https://huggingface.co/jkzfs/VITS_models/resolve/main/AISO-HOWATTO.pth\n",
    "\n",
    "#【RVCモデルの追加フォルダ作成】\n",
    "# '/notebooks/voice-generator-webui/vc/\"RVCモデルの名前\"'\n",
    "#【フォルダ構造】\n",
    "# RVCモデルの名前\n",
    "#　|— RVCモデルの名前.pth\n",
    "#　|— added.index (左記のファイル名に変更）\n",
    "#　|— total_fea.npy（左記のファイル名に変更）\n",
    "\n",
    "\n",
    "!python3 webui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d61606-3bd1-44ff-aedf-ae45366afe87",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-01-31T05:07:18.067661Z",
     "iopub.status.busy": "2024-01-31T05:07:18.067322Z",
     "iopub.status.idle": "2024-01-31T05:07:18.153281Z",
     "shell.execute_reply": "2024-01-31T05:07:18.152487Z",
     "shell.execute_reply.started": "2024-01-31T05:07:18.067637Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /notebooks/voice-generator-webui\n",
      "Config: [1025, 32, 192, 192, 768, 2, 6, 3, 0, '1', [3, 7, 11], [[1, 3, 5], [1, 3, 5], [1, 3, 5]], [10, 10, 2, 2], 512, [16, 16, 4, 4], 109, 256, 768, 40000]\n",
      "\n",
      "Parameter Shapes:\n",
      "enc_p.emb_phone.weight: torch.Size([192, 768])\n",
      "enc_p.emb_phone.bias: torch.Size([192])\n",
      "enc_p.emb_pitch.weight: torch.Size([256, 192])\n",
      "enc_p.encoder.attn_layers.0.emb_rel_k: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.0.emb_rel_v: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.0.conv_q.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.0.conv_q.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.0.conv_k.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.0.conv_k.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.0.conv_v.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.0.conv_v.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.0.conv_o.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.0.conv_o.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.1.emb_rel_k: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.1.emb_rel_v: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.1.conv_q.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.1.conv_q.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.1.conv_k.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.1.conv_k.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.1.conv_v.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.1.conv_v.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.1.conv_o.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.1.conv_o.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.2.emb_rel_k: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.2.emb_rel_v: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.2.conv_q.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.2.conv_q.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.2.conv_k.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.2.conv_k.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.2.conv_v.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.2.conv_v.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.2.conv_o.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.2.conv_o.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.3.emb_rel_k: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.3.emb_rel_v: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.3.conv_q.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.3.conv_q.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.3.conv_k.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.3.conv_k.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.3.conv_v.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.3.conv_v.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.3.conv_o.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.3.conv_o.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.4.emb_rel_k: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.4.emb_rel_v: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.4.conv_q.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.4.conv_q.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.4.conv_k.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.4.conv_k.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.4.conv_v.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.4.conv_v.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.4.conv_o.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.4.conv_o.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.5.emb_rel_k: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.5.emb_rel_v: torch.Size([1, 21, 96])\n",
      "enc_p.encoder.attn_layers.5.conv_q.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.5.conv_q.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.5.conv_k.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.5.conv_k.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.5.conv_v.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.5.conv_v.bias: torch.Size([192])\n",
      "enc_p.encoder.attn_layers.5.conv_o.weight: torch.Size([192, 192, 1])\n",
      "enc_p.encoder.attn_layers.5.conv_o.bias: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.0.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.0.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.1.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.1.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.2.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.2.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.3.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.3.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.4.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.4.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.5.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_1.5.beta: torch.Size([192])\n",
      "enc_p.encoder.ffn_layers.0.conv_1.weight: torch.Size([768, 192, 3])\n",
      "enc_p.encoder.ffn_layers.0.conv_1.bias: torch.Size([768])\n",
      "enc_p.encoder.ffn_layers.0.conv_2.weight: torch.Size([192, 768, 3])\n",
      "enc_p.encoder.ffn_layers.0.conv_2.bias: torch.Size([192])\n",
      "enc_p.encoder.ffn_layers.1.conv_1.weight: torch.Size([768, 192, 3])\n",
      "enc_p.encoder.ffn_layers.1.conv_1.bias: torch.Size([768])\n",
      "enc_p.encoder.ffn_layers.1.conv_2.weight: torch.Size([192, 768, 3])\n",
      "enc_p.encoder.ffn_layers.1.conv_2.bias: torch.Size([192])\n",
      "enc_p.encoder.ffn_layers.2.conv_1.weight: torch.Size([768, 192, 3])\n",
      "enc_p.encoder.ffn_layers.2.conv_1.bias: torch.Size([768])\n",
      "enc_p.encoder.ffn_layers.2.conv_2.weight: torch.Size([192, 768, 3])\n",
      "enc_p.encoder.ffn_layers.2.conv_2.bias: torch.Size([192])\n",
      "enc_p.encoder.ffn_layers.3.conv_1.weight: torch.Size([768, 192, 3])\n",
      "enc_p.encoder.ffn_layers.3.conv_1.bias: torch.Size([768])\n",
      "enc_p.encoder.ffn_layers.3.conv_2.weight: torch.Size([192, 768, 3])\n",
      "enc_p.encoder.ffn_layers.3.conv_2.bias: torch.Size([192])\n",
      "enc_p.encoder.ffn_layers.4.conv_1.weight: torch.Size([768, 192, 3])\n",
      "enc_p.encoder.ffn_layers.4.conv_1.bias: torch.Size([768])\n",
      "enc_p.encoder.ffn_layers.4.conv_2.weight: torch.Size([192, 768, 3])\n",
      "enc_p.encoder.ffn_layers.4.conv_2.bias: torch.Size([192])\n",
      "enc_p.encoder.ffn_layers.5.conv_1.weight: torch.Size([768, 192, 3])\n",
      "enc_p.encoder.ffn_layers.5.conv_1.bias: torch.Size([768])\n",
      "enc_p.encoder.ffn_layers.5.conv_2.weight: torch.Size([192, 768, 3])\n",
      "enc_p.encoder.ffn_layers.5.conv_2.bias: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.0.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.0.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.1.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.1.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.2.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.2.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.3.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.3.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.4.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.4.beta: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.5.gamma: torch.Size([192])\n",
      "enc_p.encoder.norm_layers_2.5.beta: torch.Size([192])\n",
      "enc_p.proj.weight: torch.Size([384, 192, 1])\n",
      "enc_p.proj.bias: torch.Size([384])\n",
      "dec.m_source.l_linear.weight: torch.Size([1, 1])\n",
      "dec.m_source.l_linear.bias: torch.Size([1])\n",
      "dec.noise_convs.0.weight: torch.Size([256, 1, 80])\n",
      "dec.noise_convs.0.bias: torch.Size([256])\n",
      "dec.noise_convs.1.weight: torch.Size([128, 1, 8])\n",
      "dec.noise_convs.1.bias: torch.Size([128])\n",
      "dec.noise_convs.2.weight: torch.Size([64, 1, 4])\n",
      "dec.noise_convs.2.bias: torch.Size([64])\n",
      "dec.noise_convs.3.weight: torch.Size([32, 1, 1])\n",
      "dec.noise_convs.3.bias: torch.Size([32])\n",
      "dec.conv_pre.weight: torch.Size([512, 192, 7])\n",
      "dec.conv_pre.bias: torch.Size([512])\n",
      "dec.ups.0.bias: torch.Size([256])\n",
      "dec.ups.0.weight_g: torch.Size([512, 1, 1])\n",
      "dec.ups.0.weight_v: torch.Size([512, 256, 16])\n",
      "dec.ups.1.bias: torch.Size([128])\n",
      "dec.ups.1.weight_g: torch.Size([256, 1, 1])\n",
      "dec.ups.1.weight_v: torch.Size([256, 128, 16])\n",
      "dec.ups.2.bias: torch.Size([64])\n",
      "dec.ups.2.weight_g: torch.Size([128, 1, 1])\n",
      "dec.ups.2.weight_v: torch.Size([128, 64, 4])\n",
      "dec.ups.3.bias: torch.Size([32])\n",
      "dec.ups.3.weight_g: torch.Size([64, 1, 1])\n",
      "dec.ups.3.weight_v: torch.Size([64, 32, 4])\n",
      "dec.resblocks.0.convs1.0.bias: torch.Size([256])\n",
      "dec.resblocks.0.convs1.0.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.0.convs1.0.weight_v: torch.Size([256, 256, 3])\n",
      "dec.resblocks.0.convs1.1.bias: torch.Size([256])\n",
      "dec.resblocks.0.convs1.1.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.0.convs1.1.weight_v: torch.Size([256, 256, 3])\n",
      "dec.resblocks.0.convs1.2.bias: torch.Size([256])\n",
      "dec.resblocks.0.convs1.2.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.0.convs1.2.weight_v: torch.Size([256, 256, 3])\n",
      "dec.resblocks.0.convs2.0.bias: torch.Size([256])\n",
      "dec.resblocks.0.convs2.0.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.0.convs2.0.weight_v: torch.Size([256, 256, 3])\n",
      "dec.resblocks.0.convs2.1.bias: torch.Size([256])\n",
      "dec.resblocks.0.convs2.1.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.0.convs2.1.weight_v: torch.Size([256, 256, 3])\n",
      "dec.resblocks.0.convs2.2.bias: torch.Size([256])\n",
      "dec.resblocks.0.convs2.2.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.0.convs2.2.weight_v: torch.Size([256, 256, 3])\n",
      "dec.resblocks.1.convs1.0.bias: torch.Size([256])\n",
      "dec.resblocks.1.convs1.0.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.1.convs1.0.weight_v: torch.Size([256, 256, 7])\n",
      "dec.resblocks.1.convs1.1.bias: torch.Size([256])\n",
      "dec.resblocks.1.convs1.1.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.1.convs1.1.weight_v: torch.Size([256, 256, 7])\n",
      "dec.resblocks.1.convs1.2.bias: torch.Size([256])\n",
      "dec.resblocks.1.convs1.2.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.1.convs1.2.weight_v: torch.Size([256, 256, 7])\n",
      "dec.resblocks.1.convs2.0.bias: torch.Size([256])\n",
      "dec.resblocks.1.convs2.0.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.1.convs2.0.weight_v: torch.Size([256, 256, 7])\n",
      "dec.resblocks.1.convs2.1.bias: torch.Size([256])\n",
      "dec.resblocks.1.convs2.1.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.1.convs2.1.weight_v: torch.Size([256, 256, 7])\n",
      "dec.resblocks.1.convs2.2.bias: torch.Size([256])\n",
      "dec.resblocks.1.convs2.2.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.1.convs2.2.weight_v: torch.Size([256, 256, 7])\n",
      "dec.resblocks.2.convs1.0.bias: torch.Size([256])\n",
      "dec.resblocks.2.convs1.0.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.2.convs1.0.weight_v: torch.Size([256, 256, 11])\n",
      "dec.resblocks.2.convs1.1.bias: torch.Size([256])\n",
      "dec.resblocks.2.convs1.1.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.2.convs1.1.weight_v: torch.Size([256, 256, 11])\n",
      "dec.resblocks.2.convs1.2.bias: torch.Size([256])\n",
      "dec.resblocks.2.convs1.2.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.2.convs1.2.weight_v: torch.Size([256, 256, 11])\n",
      "dec.resblocks.2.convs2.0.bias: torch.Size([256])\n",
      "dec.resblocks.2.convs2.0.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.2.convs2.0.weight_v: torch.Size([256, 256, 11])\n",
      "dec.resblocks.2.convs2.1.bias: torch.Size([256])\n",
      "dec.resblocks.2.convs2.1.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.2.convs2.1.weight_v: torch.Size([256, 256, 11])\n",
      "dec.resblocks.2.convs2.2.bias: torch.Size([256])\n",
      "dec.resblocks.2.convs2.2.weight_g: torch.Size([256, 1, 1])\n",
      "dec.resblocks.2.convs2.2.weight_v: torch.Size([256, 256, 11])\n",
      "dec.resblocks.3.convs1.0.bias: torch.Size([128])\n",
      "dec.resblocks.3.convs1.0.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.3.convs1.0.weight_v: torch.Size([128, 128, 3])\n",
      "dec.resblocks.3.convs1.1.bias: torch.Size([128])\n",
      "dec.resblocks.3.convs1.1.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.3.convs1.1.weight_v: torch.Size([128, 128, 3])\n",
      "dec.resblocks.3.convs1.2.bias: torch.Size([128])\n",
      "dec.resblocks.3.convs1.2.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.3.convs1.2.weight_v: torch.Size([128, 128, 3])\n",
      "dec.resblocks.3.convs2.0.bias: torch.Size([128])\n",
      "dec.resblocks.3.convs2.0.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.3.convs2.0.weight_v: torch.Size([128, 128, 3])\n",
      "dec.resblocks.3.convs2.1.bias: torch.Size([128])\n",
      "dec.resblocks.3.convs2.1.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.3.convs2.1.weight_v: torch.Size([128, 128, 3])\n",
      "dec.resblocks.3.convs2.2.bias: torch.Size([128])\n",
      "dec.resblocks.3.convs2.2.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.3.convs2.2.weight_v: torch.Size([128, 128, 3])\n",
      "dec.resblocks.4.convs1.0.bias: torch.Size([128])\n",
      "dec.resblocks.4.convs1.0.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.4.convs1.0.weight_v: torch.Size([128, 128, 7])\n",
      "dec.resblocks.4.convs1.1.bias: torch.Size([128])\n",
      "dec.resblocks.4.convs1.1.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.4.convs1.1.weight_v: torch.Size([128, 128, 7])\n",
      "dec.resblocks.4.convs1.2.bias: torch.Size([128])\n",
      "dec.resblocks.4.convs1.2.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.4.convs1.2.weight_v: torch.Size([128, 128, 7])\n",
      "dec.resblocks.4.convs2.0.bias: torch.Size([128])\n",
      "dec.resblocks.4.convs2.0.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.4.convs2.0.weight_v: torch.Size([128, 128, 7])\n",
      "dec.resblocks.4.convs2.1.bias: torch.Size([128])\n",
      "dec.resblocks.4.convs2.1.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.4.convs2.1.weight_v: torch.Size([128, 128, 7])\n",
      "dec.resblocks.4.convs2.2.bias: torch.Size([128])\n",
      "dec.resblocks.4.convs2.2.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.4.convs2.2.weight_v: torch.Size([128, 128, 7])\n",
      "dec.resblocks.5.convs1.0.bias: torch.Size([128])\n",
      "dec.resblocks.5.convs1.0.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.5.convs1.0.weight_v: torch.Size([128, 128, 11])\n",
      "dec.resblocks.5.convs1.1.bias: torch.Size([128])\n",
      "dec.resblocks.5.convs1.1.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.5.convs1.1.weight_v: torch.Size([128, 128, 11])\n",
      "dec.resblocks.5.convs1.2.bias: torch.Size([128])\n",
      "dec.resblocks.5.convs1.2.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.5.convs1.2.weight_v: torch.Size([128, 128, 11])\n",
      "dec.resblocks.5.convs2.0.bias: torch.Size([128])\n",
      "dec.resblocks.5.convs2.0.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.5.convs2.0.weight_v: torch.Size([128, 128, 11])\n",
      "dec.resblocks.5.convs2.1.bias: torch.Size([128])\n",
      "dec.resblocks.5.convs2.1.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.5.convs2.1.weight_v: torch.Size([128, 128, 11])\n",
      "dec.resblocks.5.convs2.2.bias: torch.Size([128])\n",
      "dec.resblocks.5.convs2.2.weight_g: torch.Size([128, 1, 1])\n",
      "dec.resblocks.5.convs2.2.weight_v: torch.Size([128, 128, 11])\n",
      "dec.resblocks.6.convs1.0.bias: torch.Size([64])\n",
      "dec.resblocks.6.convs1.0.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.6.convs1.0.weight_v: torch.Size([64, 64, 3])\n",
      "dec.resblocks.6.convs1.1.bias: torch.Size([64])\n",
      "dec.resblocks.6.convs1.1.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.6.convs1.1.weight_v: torch.Size([64, 64, 3])\n",
      "dec.resblocks.6.convs1.2.bias: torch.Size([64])\n",
      "dec.resblocks.6.convs1.2.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.6.convs1.2.weight_v: torch.Size([64, 64, 3])\n",
      "dec.resblocks.6.convs2.0.bias: torch.Size([64])\n",
      "dec.resblocks.6.convs2.0.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.6.convs2.0.weight_v: torch.Size([64, 64, 3])\n",
      "dec.resblocks.6.convs2.1.bias: torch.Size([64])\n",
      "dec.resblocks.6.convs2.1.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.6.convs2.1.weight_v: torch.Size([64, 64, 3])\n",
      "dec.resblocks.6.convs2.2.bias: torch.Size([64])\n",
      "dec.resblocks.6.convs2.2.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.6.convs2.2.weight_v: torch.Size([64, 64, 3])\n",
      "dec.resblocks.7.convs1.0.bias: torch.Size([64])\n",
      "dec.resblocks.7.convs1.0.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.7.convs1.0.weight_v: torch.Size([64, 64, 7])\n",
      "dec.resblocks.7.convs1.1.bias: torch.Size([64])\n",
      "dec.resblocks.7.convs1.1.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.7.convs1.1.weight_v: torch.Size([64, 64, 7])\n",
      "dec.resblocks.7.convs1.2.bias: torch.Size([64])\n",
      "dec.resblocks.7.convs1.2.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.7.convs1.2.weight_v: torch.Size([64, 64, 7])\n",
      "dec.resblocks.7.convs2.0.bias: torch.Size([64])\n",
      "dec.resblocks.7.convs2.0.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.7.convs2.0.weight_v: torch.Size([64, 64, 7])\n",
      "dec.resblocks.7.convs2.1.bias: torch.Size([64])\n",
      "dec.resblocks.7.convs2.1.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.7.convs2.1.weight_v: torch.Size([64, 64, 7])\n",
      "dec.resblocks.7.convs2.2.bias: torch.Size([64])\n",
      "dec.resblocks.7.convs2.2.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.7.convs2.2.weight_v: torch.Size([64, 64, 7])\n",
      "dec.resblocks.8.convs1.0.bias: torch.Size([64])\n",
      "dec.resblocks.8.convs1.0.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.8.convs1.0.weight_v: torch.Size([64, 64, 11])\n",
      "dec.resblocks.8.convs1.1.bias: torch.Size([64])\n",
      "dec.resblocks.8.convs1.1.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.8.convs1.1.weight_v: torch.Size([64, 64, 11])\n",
      "dec.resblocks.8.convs1.2.bias: torch.Size([64])\n",
      "dec.resblocks.8.convs1.2.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.8.convs1.2.weight_v: torch.Size([64, 64, 11])\n",
      "dec.resblocks.8.convs2.0.bias: torch.Size([64])\n",
      "dec.resblocks.8.convs2.0.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.8.convs2.0.weight_v: torch.Size([64, 64, 11])\n",
      "dec.resblocks.8.convs2.1.bias: torch.Size([64])\n",
      "dec.resblocks.8.convs2.1.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.8.convs2.1.weight_v: torch.Size([64, 64, 11])\n",
      "dec.resblocks.8.convs2.2.bias: torch.Size([64])\n",
      "dec.resblocks.8.convs2.2.weight_g: torch.Size([64, 1, 1])\n",
      "dec.resblocks.8.convs2.2.weight_v: torch.Size([64, 64, 11])\n",
      "dec.resblocks.9.convs1.0.bias: torch.Size([32])\n",
      "dec.resblocks.9.convs1.0.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.9.convs1.0.weight_v: torch.Size([32, 32, 3])\n",
      "dec.resblocks.9.convs1.1.bias: torch.Size([32])\n",
      "dec.resblocks.9.convs1.1.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.9.convs1.1.weight_v: torch.Size([32, 32, 3])\n",
      "dec.resblocks.9.convs1.2.bias: torch.Size([32])\n",
      "dec.resblocks.9.convs1.2.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.9.convs1.2.weight_v: torch.Size([32, 32, 3])\n",
      "dec.resblocks.9.convs2.0.bias: torch.Size([32])\n",
      "dec.resblocks.9.convs2.0.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.9.convs2.0.weight_v: torch.Size([32, 32, 3])\n",
      "dec.resblocks.9.convs2.1.bias: torch.Size([32])\n",
      "dec.resblocks.9.convs2.1.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.9.convs2.1.weight_v: torch.Size([32, 32, 3])\n",
      "dec.resblocks.9.convs2.2.bias: torch.Size([32])\n",
      "dec.resblocks.9.convs2.2.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.9.convs2.2.weight_v: torch.Size([32, 32, 3])\n",
      "dec.resblocks.10.convs1.0.bias: torch.Size([32])\n",
      "dec.resblocks.10.convs1.0.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.10.convs1.0.weight_v: torch.Size([32, 32, 7])\n",
      "dec.resblocks.10.convs1.1.bias: torch.Size([32])\n",
      "dec.resblocks.10.convs1.1.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.10.convs1.1.weight_v: torch.Size([32, 32, 7])\n",
      "dec.resblocks.10.convs1.2.bias: torch.Size([32])\n",
      "dec.resblocks.10.convs1.2.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.10.convs1.2.weight_v: torch.Size([32, 32, 7])\n",
      "dec.resblocks.10.convs2.0.bias: torch.Size([32])\n",
      "dec.resblocks.10.convs2.0.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.10.convs2.0.weight_v: torch.Size([32, 32, 7])\n",
      "dec.resblocks.10.convs2.1.bias: torch.Size([32])\n",
      "dec.resblocks.10.convs2.1.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.10.convs2.1.weight_v: torch.Size([32, 32, 7])\n",
      "dec.resblocks.10.convs2.2.bias: torch.Size([32])\n",
      "dec.resblocks.10.convs2.2.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.10.convs2.2.weight_v: torch.Size([32, 32, 7])\n",
      "dec.resblocks.11.convs1.0.bias: torch.Size([32])\n",
      "dec.resblocks.11.convs1.0.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.11.convs1.0.weight_v: torch.Size([32, 32, 11])\n",
      "dec.resblocks.11.convs1.1.bias: torch.Size([32])\n",
      "dec.resblocks.11.convs1.1.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.11.convs1.1.weight_v: torch.Size([32, 32, 11])\n",
      "dec.resblocks.11.convs1.2.bias: torch.Size([32])\n",
      "dec.resblocks.11.convs1.2.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.11.convs1.2.weight_v: torch.Size([32, 32, 11])\n",
      "dec.resblocks.11.convs2.0.bias: torch.Size([32])\n",
      "dec.resblocks.11.convs2.0.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.11.convs2.0.weight_v: torch.Size([32, 32, 11])\n",
      "dec.resblocks.11.convs2.1.bias: torch.Size([32])\n",
      "dec.resblocks.11.convs2.1.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.11.convs2.1.weight_v: torch.Size([32, 32, 11])\n",
      "dec.resblocks.11.convs2.2.bias: torch.Size([32])\n",
      "dec.resblocks.11.convs2.2.weight_g: torch.Size([32, 1, 1])\n",
      "dec.resblocks.11.convs2.2.weight_v: torch.Size([32, 32, 11])\n",
      "dec.conv_post.weight: torch.Size([1, 32, 7])\n",
      "dec.cond.weight: torch.Size([512, 256, 1])\n",
      "dec.cond.bias: torch.Size([512])\n",
      "flow.flows.0.pre.weight: torch.Size([192, 96, 1])\n",
      "flow.flows.0.pre.bias: torch.Size([192])\n",
      "flow.flows.0.enc.in_layers.0.bias: torch.Size([384])\n",
      "flow.flows.0.enc.in_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.0.enc.in_layers.0.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.0.enc.in_layers.1.bias: torch.Size([384])\n",
      "flow.flows.0.enc.in_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.0.enc.in_layers.1.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.0.enc.in_layers.2.bias: torch.Size([384])\n",
      "flow.flows.0.enc.in_layers.2.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.0.enc.in_layers.2.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.0.enc.res_skip_layers.0.bias: torch.Size([384])\n",
      "flow.flows.0.enc.res_skip_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.0.enc.res_skip_layers.0.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.0.enc.res_skip_layers.1.bias: torch.Size([384])\n",
      "flow.flows.0.enc.res_skip_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.0.enc.res_skip_layers.1.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.0.enc.res_skip_layers.2.bias: torch.Size([192])\n",
      "flow.flows.0.enc.res_skip_layers.2.weight_g: torch.Size([192, 1, 1])\n",
      "flow.flows.0.enc.res_skip_layers.2.weight_v: torch.Size([192, 192, 1])\n",
      "flow.flows.0.enc.cond_layer.bias: torch.Size([1152])\n",
      "flow.flows.0.enc.cond_layer.weight_g: torch.Size([1152, 1, 1])\n",
      "flow.flows.0.enc.cond_layer.weight_v: torch.Size([1152, 256, 1])\n",
      "flow.flows.0.post.weight: torch.Size([96, 192, 1])\n",
      "flow.flows.0.post.bias: torch.Size([96])\n",
      "flow.flows.2.pre.weight: torch.Size([192, 96, 1])\n",
      "flow.flows.2.pre.bias: torch.Size([192])\n",
      "flow.flows.2.enc.in_layers.0.bias: torch.Size([384])\n",
      "flow.flows.2.enc.in_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.2.enc.in_layers.0.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.2.enc.in_layers.1.bias: torch.Size([384])\n",
      "flow.flows.2.enc.in_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.2.enc.in_layers.1.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.2.enc.in_layers.2.bias: torch.Size([384])\n",
      "flow.flows.2.enc.in_layers.2.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.2.enc.in_layers.2.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.2.enc.res_skip_layers.0.bias: torch.Size([384])\n",
      "flow.flows.2.enc.res_skip_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.2.enc.res_skip_layers.0.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.2.enc.res_skip_layers.1.bias: torch.Size([384])\n",
      "flow.flows.2.enc.res_skip_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.2.enc.res_skip_layers.1.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.2.enc.res_skip_layers.2.bias: torch.Size([192])\n",
      "flow.flows.2.enc.res_skip_layers.2.weight_g: torch.Size([192, 1, 1])\n",
      "flow.flows.2.enc.res_skip_layers.2.weight_v: torch.Size([192, 192, 1])\n",
      "flow.flows.2.enc.cond_layer.bias: torch.Size([1152])\n",
      "flow.flows.2.enc.cond_layer.weight_g: torch.Size([1152, 1, 1])\n",
      "flow.flows.2.enc.cond_layer.weight_v: torch.Size([1152, 256, 1])\n",
      "flow.flows.2.post.weight: torch.Size([96, 192, 1])\n",
      "flow.flows.2.post.bias: torch.Size([96])\n",
      "flow.flows.4.pre.weight: torch.Size([192, 96, 1])\n",
      "flow.flows.4.pre.bias: torch.Size([192])\n",
      "flow.flows.4.enc.in_layers.0.bias: torch.Size([384])\n",
      "flow.flows.4.enc.in_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.4.enc.in_layers.0.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.4.enc.in_layers.1.bias: torch.Size([384])\n",
      "flow.flows.4.enc.in_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.4.enc.in_layers.1.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.4.enc.in_layers.2.bias: torch.Size([384])\n",
      "flow.flows.4.enc.in_layers.2.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.4.enc.in_layers.2.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.4.enc.res_skip_layers.0.bias: torch.Size([384])\n",
      "flow.flows.4.enc.res_skip_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.4.enc.res_skip_layers.0.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.4.enc.res_skip_layers.1.bias: torch.Size([384])\n",
      "flow.flows.4.enc.res_skip_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.4.enc.res_skip_layers.1.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.4.enc.res_skip_layers.2.bias: torch.Size([192])\n",
      "flow.flows.4.enc.res_skip_layers.2.weight_g: torch.Size([192, 1, 1])\n",
      "flow.flows.4.enc.res_skip_layers.2.weight_v: torch.Size([192, 192, 1])\n",
      "flow.flows.4.enc.cond_layer.bias: torch.Size([1152])\n",
      "flow.flows.4.enc.cond_layer.weight_g: torch.Size([1152, 1, 1])\n",
      "flow.flows.4.enc.cond_layer.weight_v: torch.Size([1152, 256, 1])\n",
      "flow.flows.4.post.weight: torch.Size([96, 192, 1])\n",
      "flow.flows.4.post.bias: torch.Size([96])\n",
      "flow.flows.6.pre.weight: torch.Size([192, 96, 1])\n",
      "flow.flows.6.pre.bias: torch.Size([192])\n",
      "flow.flows.6.enc.in_layers.0.bias: torch.Size([384])\n",
      "flow.flows.6.enc.in_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.6.enc.in_layers.0.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.6.enc.in_layers.1.bias: torch.Size([384])\n",
      "flow.flows.6.enc.in_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.6.enc.in_layers.1.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.6.enc.in_layers.2.bias: torch.Size([384])\n",
      "flow.flows.6.enc.in_layers.2.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.6.enc.in_layers.2.weight_v: torch.Size([384, 192, 5])\n",
      "flow.flows.6.enc.res_skip_layers.0.bias: torch.Size([384])\n",
      "flow.flows.6.enc.res_skip_layers.0.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.6.enc.res_skip_layers.0.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.6.enc.res_skip_layers.1.bias: torch.Size([384])\n",
      "flow.flows.6.enc.res_skip_layers.1.weight_g: torch.Size([384, 1, 1])\n",
      "flow.flows.6.enc.res_skip_layers.1.weight_v: torch.Size([384, 192, 1])\n",
      "flow.flows.6.enc.res_skip_layers.2.bias: torch.Size([192])\n",
      "flow.flows.6.enc.res_skip_layers.2.weight_g: torch.Size([192, 1, 1])\n",
      "flow.flows.6.enc.res_skip_layers.2.weight_v: torch.Size([192, 192, 1])\n",
      "flow.flows.6.enc.cond_layer.bias: torch.Size([1152])\n",
      "flow.flows.6.enc.cond_layer.weight_g: torch.Size([1152, 1, 1])\n",
      "flow.flows.6.enc.cond_layer.weight_v: torch.Size([1152, 256, 1])\n",
      "flow.flows.6.post.weight: torch.Size([96, 192, 1])\n",
      "flow.flows.6.post.bias: torch.Size([96])\n",
      "emb_g.weight: torch.Size([109, 256])\n"
     ]
    }
   ],
   "source": [
    "#チェックポイントの形状確認\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 現在のカレントディレクトリを出力\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "# チェックポイントファイルの絶対パスを指定\n",
    "checkpoint_path = '/notebooks/voice-generator-webui/vc/models/RVCモデルの名前/RVCモデルの名前.pth'\n",
    "\n",
    "# チェックポイントファイルをロード\n",
    "cpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "# config の内容を表示\n",
    "print(\"Config:\", cpt['config'])\n",
    "\n",
    "# チェックポイント内のすべてのパラメータの形状を表示\n",
    "print(\"\\nParameter Shapes:\")\n",
    "for param in cpt['weight']:\n",
    "    print(f\"{param}: {cpt['weight'][param].shape}\")\n",
    "    \n",
    "checkpoint = torch.load('/notebooks/voice-generator-webui/vc/models/RVCモデルの名前/RVCモデルの名前.pth', map_location='cpu')\n",
    "for key, value in checkpoint.items():\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63dea61e-96df-4fd5-9a32-822341cf165f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-02-19T08:32:48.622601Z",
     "iopub.status.busy": "2024-02-19T08:32:48.622385Z",
     "iopub.status.idle": "2024-02-19T08:32:49.415190Z",
     "shell.execute_reply": "2024-02-19T08:32:49.413824Z",
     "shell.execute_reply.started": "2024-02-19T08:32:48.622582Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index dimension: 768\n",
      "Query vector shape: (10, 768)\n",
      "Search results: [[31729 39266 39945 61627 18686]\n",
      " [23725 30426 14056 46832 39945]\n",
      " [43588    -1    -1    -1    -1]\n",
      " [10175 43207 19088 14206 37202]\n",
      " [11774 46832 23725 30426 14056]\n",
      " [10175 19088 43207 54096 27303]\n",
      " [10175 19088 43207 54096 31729]\n",
      " [10175 43207 19088 37202  5769]\n",
      " [44625 37202 14206  4521 43207]\n",
      " [14056 43207 10175 19088 23725]]\n",
      "Big npy shape: (64411, 768)\n",
      "Indexes are within range of big npy: True\n",
      "Memory size of big npy (MB): 188.7042236328125\n"
     ]
    }
   ],
   "source": [
    "# インデックスファイルの形状確認\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# インデックスを読み込む\n",
    "index = faiss.read_index('/notebooks/voice-generator-webui/vc/models/RVCモデルの名前/added.index')\n",
    "\n",
    "# インデックスの次元数を確認\n",
    "print(\"Index dimension:\", index.d)\n",
    "\n",
    "# クエリベクトルを準備（ここではサンプルとしてランダムなベクトルを生成）\n",
    "npy = np.random.rand(10, index.d).astype('float32')  # 10個のクエリを想定、float32にキャスト\n",
    "\n",
    "# クエリベクトルの形状を確認\n",
    "print(\"Query vector shape:\", npy.shape)\n",
    "\n",
    "# 検索を実行して結果を確認\n",
    "D, I = index.search(npy, 5)  # 最も近い5個のエントリを検索\n",
    "print(\"Search results:\", I)\n",
    "\n",
    "# big_npy 配列のロード（サンプルとしてランダムなデータを生成）\n",
    "# 実際には、big_npy 配列のデータを読み込む必要があります\n",
    "big_npy = np.random.rand(index.ntotal, index.d).astype('float32')\n",
    "\n",
    "# big_npy 配列の形状を確認\n",
    "print(\"Big npy shape:\", big_npy.shape)\n",
    "\n",
    "# インデックスの範囲内であることを確認\n",
    "print(\"Indexes are within range of big npy:\", np.all(I < big_npy.shape[0]))\n",
    "\n",
    "# メモリ使用量を確認（オプショナル）\n",
    "import sys\n",
    "print(\"Memory size of big npy (MB):\", sys.getsizeof(big_npy) / (1024 * 1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
